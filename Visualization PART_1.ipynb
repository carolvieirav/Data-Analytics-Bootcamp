{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PART_1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carolvieirav/Labs-and-Projects-Ironhack---DTFT-2020-Jun-/blob/master/Visualization%20PART_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5KieLeIparnW"
      },
      "source": [
        "#  PART 1 - Web Scraping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u86CgeHKbpeK"
      },
      "source": [
        "# **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ml-KXLncarnb",
        "colab": {}
      },
      "source": [
        "# import all libraries selenium related\n",
        "\n",
        "from __future__ import print_function\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import (NoSuchElementException , ElementClickInterceptedException, TimeoutException)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vc174sMDarnw",
        "colab": {}
      },
      "source": [
        "# import all libraries for link extraction \n",
        "\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen\n",
        "import lxml\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0poHVIp4aroH",
        "colab": {}
      },
      "source": [
        "from settings import search_keys\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "import datetime\n",
        "import argparse\n",
        "import matplotlib as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EDmXGih4aroV",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "\n",
        "pd.options.display.max_columns = None\n",
        "pd.options.display.max_rows = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dgFeBmZEaroh"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w6emq7THax44"
      },
      "source": [
        "#  **Glassdoor** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9GZLUwWMarol"
      },
      "source": [
        "Glassdoor is a website where current and former employees anonymously review companies. Glassdoor also allows users to anonymously submit and view salaries as well as search and apply for jobs on its platform.\n",
        "\n",
        "(from wikipedia)\n",
        "\n",
        "\n",
        "![imagem.png](attachment:imagem.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nNhpmUrxaron",
        "colab": {}
      },
      "source": [
        "# Specifyinng number of jobs as an excessive number of requests made by the computer IP can potentially block you\n",
        "# glassdoor has 30 listing per page, so we calculated manually how much is the total amount of jobs present \n",
        "# i.e (num_pages*30)\n",
        "\n",
        "num_jobs = 150\n",
        "start_pg =1 \n",
        "end_pg = 30\n",
        "total_pages = 30\n",
        "\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage') \n",
        "driver = webdriver.Chrome('chromedriver' , options = options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NXC-Cwdtaro2",
        "colab": {}
      },
      "source": [
        "def scrape_links(driver , start_pg , end_pg ,total_pages):\n",
        "    \n",
        "    ''' \n",
        "    \n",
        "    This function uses selenium and beautiful soup(BS4) to extract the page source.\n",
        "    \n",
        "    '''\n",
        "     \n",
        "    url_head = 'https://www.glassdoor.com.br'\n",
        "    scr_links = [] \n",
        "    \n",
        "    # Total pages are the total web pages on the website.\n",
        "    if end_pg > total_pages:\n",
        "        print(\"Enter valid Page Indexes\")\n",
        "    #br = self.Initialize_Driver()\n",
        "    br = driver\n",
        "    for pg_num in range(start_pg, end_pg+1):\n",
        "        url = 'https://www.glassdoor.com.br/Vaga/s%C3%A3o-paulo-data-scientist-vagas-SRCH_IL.0,9_IC2479061_KO10,24'+'.htm?radius=124'\n",
        "        br.get(url)\n",
        "        br.implicitly_wait(5)\n",
        "        \n",
        "        soup = BeautifulSoup(br.page_source , 'lxml')\n",
        "        #print(f\"Scraping Page Number : {pg_num}\")      \n",
        "        for a_tag in soup.find_all('a', {'class':'jobInfoItem jobTitle jobLink'}):\n",
        "            scr_links.append(a_tag['href'])\n",
        "            \n",
        "            \n",
        "    op_links = list(set(scr_links))\n",
        "    if len(op_links) == ((end_pg-start_pg)+1)*30:\n",
        "        print(\"Scraping of links is completed\")\n",
        "        \n",
        "        return op_links\n",
        "    \n",
        "    \n",
        "    \n",
        "def scrape_data(num_jobs, driver, links):\n",
        "    \n",
        "    #data_df = df\n",
        "    #cleaned_links = scrape_links(br)\n",
        "    #if data_df is None:\n",
        "    data_df = pd.DataFrame(columns = ['Job_title' , 'Company' , 'State' , 'City', 'Job_Desc' , \n",
        "                                           'Industry' , 'Date_Posted' , 'Valid_until' , 'Job_Type' ,\n",
        "                                      ])\n",
        "    br = driver\n",
        "    start = time.time()\n",
        "    print(\"Gathering Information\")\n",
        "    \n",
        "    br = driver\n",
        "    for job in range(num_jobs):\n",
        "        br.get(links[job])\n",
        "        br.implicitly_wait(10)\n",
        "        soup = BeautifulSoup(br.page_source , 'lxml')\n",
        "\n",
        "        try:\n",
        "            # the rest of the data is available in scrip = 'application/ld+json' i.e in  json file\n",
        "            json_file = soup.find('script' , {'type':'application/ld+json'})\n",
        "            op_json = json.loads(str(json_file.text) , strict = False)\n",
        "                \n",
        "            #title\n",
        "            try:\n",
        "                title = op_json['title']\n",
        "                print(title)\n",
        "            except:\n",
        "                title  = None\n",
        "                \n",
        "            # dateposted\n",
        "            try:\n",
        "                post_date = op_json['datePosted']\n",
        "            except:\n",
        "                post_date = None\n",
        "                \n",
        "            #type\n",
        "            try:\n",
        "                job_type = op_json['employmentType']\n",
        "            except:\n",
        "                job_type = None\n",
        "\n",
        "                \n",
        "            #Job Posting validity date(Y-M-D)\n",
        "            try:\n",
        "                valid_date = op_json['validThrough']\n",
        "            except:\n",
        "                valid_date = None\n",
        "                \n",
        "            #Industry\n",
        "            try:\n",
        "                industry = op_json['industry']\n",
        "            except:\n",
        "                industry = None\n",
        "                \n",
        "            #Location\n",
        "            try:\n",
        "                city = op_json['jobLocation']['address']['addressLocality']\n",
        "                state = op_json['jobLocation']['address']['addressRegion']\n",
        "            except:\n",
        "                city , state = None , None\n",
        "\n",
        "            #Company\n",
        "            try:\n",
        "                company = op_json['hiringOrganization']['name']\n",
        "            except:\n",
        "                company = None\n",
        "                \n",
        "            #Let's get Description\n",
        "            try:\n",
        "                desc = soup.find(class_ = 'desc').text\n",
        "            except:\n",
        "                desc = None\n",
        "            try:\n",
        "                rating = soup.find('span' , {'class' : 'css-nimzbu e1eh6fgm0'}).text.replace('*' , '')\n",
        "                \n",
        "            except:\n",
        "                rating = None\n",
        "\n",
        "\n",
        "            \n",
        "            data_df = data_df.append({'Job_title' : title,\n",
        "                                      'Company' : company,\n",
        "                                      'State' : state,\n",
        "                                      'City' : city,\n",
        "                                      'Job_Desc' : desc,\n",
        "                                      'Industry':industry,\n",
        "                                      'Date_Posted' : post_date,\n",
        "                                      'Valid_until' : valid_date,\n",
        "                                      'Job_Type' :job_type\n",
        "                                       } , ignore_index = True)\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "    # driver.close()\n",
        "    print(f\"Scraping Completed for {data_df.shape[0]} jobs\")\n",
        "    print(f\"Time Required : {time.time() - start} seconds\")\n",
        "        \n",
        "    return data_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q-hV1l_harpA",
        "scrolled": false,
        "colab": {},
        "outputId": "7ff92c2e-c425-449d-f437-0edc0407b193"
      },
      "source": [
        "total_jobs = 30*(end_pg - start_pg+1)\n",
        "f_df = pd.DataFrame(columns = ['Job_title', 'Company','State' ,'City', 'Job_Desc' , \n",
        "                                'Industry', 'Date_Posted', 'Valid_until', 'Job_Type'])\n",
        "br = driver\n",
        "links = scrape_links(br, start_pg, end_pg, total_pages)\n",
        "dummy_df = scrape_data(10 , br, links)\n",
        "\n",
        "# exctract 150 jobs every 1 mins \n",
        "\n",
        "#for i in range(total_jobs//num_jobs):\n",
        "    #df = scrape_data(num_jobs , driver , links[num_jobs*i : num_jobs*(i+1)])\n",
        "    #f_df = f_df.append(df)\n",
        "    #time.sleep(30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scraping of links is completed\n",
            "Gathering Information\n",
            "Scraping Completed for 0 jobs\n",
            "Time Required : 47.17237043380737 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S7Zh-gGdarpU",
        "colab": {}
      },
      "source": [
        "# save the file to csv\n",
        "\n",
        "f_df.to_csv('data_job_sp' , index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpllIYcdmsIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.read_csv('data_job_sp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CFxt7pjgbdWV"
      },
      "source": [
        "# **Linkedin**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6k0lwninarpm"
      },
      "source": [
        "LinkedIn is an American business and employment-oriented online service that operates via websites and mobile apps. Launched on May 5, 2003, it is mainly used for professional networking, including employers posting jobs and job seekers posting their CVs.\n",
        "\n",
        "![imagem.png](attachment:imagem.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4I5M3CIBarpo",
        "colab": {}
      },
      "source": [
        "def job_id(driver):\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    grabs the meta linkedin unique job id from the url\n",
        "    e.g. url = https://www.linkedin.com/jobs/view/161251904\n",
        "    returns 161251904\n",
        "    \n",
        "    \"\"\"\n",
        "    elem = driver.find_element_by_xpath(\"//meta[@property='og:url']\")\n",
        "    url  = elem.get_attribute(\"content\")\n",
        "    return url[url.find('/', 34) + 1:]\n",
        "\n",
        "def parse_post_age(text):\n",
        "    \n",
        "        \"\"\" \n",
        "        \n",
        "        map 'posted 10 days ago' => '10' \n",
        "        \n",
        "        \"\"\"\n",
        "        if 'hours' in text:\n",
        "            return '1'\n",
        "        return ''.join(list(filter(lambda c: c.isdigit(), text)))\n",
        "\n",
        "def post_data(driver):\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    get post age and page views and trim excess words\n",
        "    so that 'posted 10 days ago' becomes '10'\n",
        "    and '63 views' becomes '63' \n",
        "    \n",
        "    \"\"\"\n",
        "    post_info = {\n",
        "        \"post_age\"   : \"li.posted\", \n",
        "        \"page_views\" : \"ul.posting-info li.views\"\n",
        "    }\n",
        "    for key, selector in post_info.items():\n",
        "        try:\n",
        "            text = driver.find_element_by_css_selector(selector).text\n",
        "            if key == \"post_age\":\n",
        "                post_info[key] = parse_post_age(text)\n",
        "            else:\n",
        "                post_info[key] = ''.join(list(filter(lambda c: c.isdigit(), text)))\n",
        "        except Exception as e:\n",
        "            post_info[key] = \"\"\n",
        "            pass\n",
        "    return post_info\n",
        "\n",
        "def job_data(driver):\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    scrapes the posting info for title, company, post age, location,\n",
        "    and page views. Have seen many strange errors surrounding the\n",
        "    job tite, company, location data, so have used many try-except\n",
        "    statements to avoid potential errors with unicode, etc.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    job_info = {\n",
        "        \"job_title\"        :  \"h1.title\",\n",
        "        \"company\"          :  \"span.company\",\n",
        "        \"location\"         :  \"h3.location\",\n",
        "        \"employment_type\"  :  \"div.employment div.content div.rich-text\",\n",
        "        \"industry\"         :  \"div.industry div.content div.rich-text\",\n",
        "        \"experience\"       :  \"div.experience div.content div.rich-text\",\n",
        "        \"job_function\"     :  \"div.function div.content div.rich-text\",\n",
        "        \"description\"      :  \"div.summary div.content div.description-section div.rich-text\"\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        driver.find_element_by_css_selector(\"button#job-details-reveal\").click()\n",
        "    except Exception as e:\n",
        "        print(\"error in attempting to click 'reveal details' button\")\n",
        "        print(e)\n",
        "    for key, selector in job_info.items():\n",
        "        try:\n",
        "            job_info[key] = driver.find_element_by_css_selector(selector).text\n",
        "        except Exception as e:\n",
        "            job_info[key] = \"\"\n",
        "            pass\n",
        "    return job_info\n",
        "\n",
        "def company_data(driver):\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    return company insights, number of employees and average tenure\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        stats_selector = \"ul.company-growth-stats.stats-list li\"\n",
        "        company_stats  = driver.find_elements_by_css_selector(stats_selector)\n",
        "        company_info   = [stat.text for stat in company_stats]\n",
        "    except Exception as e:\n",
        "        print(\"error acquiring company info\")\n",
        "        print(e)\n",
        "    else:\n",
        "        try:\n",
        "            employees     = list(filter(lambda text: 'employees' in text, company_info))\n",
        "            num_employees = ''.join(list(filter(lambda c: c.isdigit(), employees[0])))\n",
        "        except Exception as e:\n",
        "            num_employees = \"\"\n",
        "            pass\n",
        "        try:\n",
        "            tenure        = list(filter(lambda text: 'tenure' in text, company_info))\n",
        "            avg_tenure    = ''.join(list(filter(lambda c: c in '0123456789.', tenure[0])))\n",
        "        except Exception as e:\n",
        "            avg_tenure    = \"\"\n",
        "            pass\n",
        "        company_info  = {\n",
        "            \"avg_tenure\"    : avg_tenure, \n",
        "            \"num_employees\" : num_employees\n",
        "        }\n",
        "    return {\"avg_tenure\" : avg_tenure, \"num_employees\" : num_employees}\n",
        "\n",
        "\n",
        "def salary_data(driver):\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    scrapes the salary info chart on the right panel returns lower, \n",
        "    upper bounds on salary estimate as well as average salary\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        _base = driver.find_element_by_xpath('/descendant::p[@class=\"salary-data-amount\"][1]').text\n",
        "        _total = driver.find_element_by_xpath('/descendant::p[@class=\"salary-data-amount\"][2]').text\n",
        "        _base_range = driver.find_element_by_xpath('/descendant::p[@class=\"salary-data-range\"][1]').text\n",
        "        _total_range = driver.find_element_by_xpath('/descendant::p[@class=\"salary-data-range\"][2]').text\n",
        "        return {\n",
        "            \"base\" : ''.join(list(filter(lambda c: c.isdigit(), _base))),\n",
        "            \"total\" : ''.join(list(filter(lambda c: c.isdigit(), _total))),\n",
        "            \"base_range\": _base_range,\n",
        "            \"total_range\": _total_range\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(\"error acquiring salary info\")\n",
        "        print(e)\n",
        "        pass\n",
        "    return {\"base\": \"\", \"total\": \"\", \"base_range\": \"\", \"total_range\": \"\"}\n",
        "\n",
        "def num_applicants(driver):\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    Grabs number of applicants from either the header of the \n",
        "    applicants-insights div, or within the applicants-table in the same \n",
        "    div element. Returns empty string if data is not available.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    # use two selectors since LI has two methods of showing number\n",
        "    # of applicants in the applicants-insights driver\n",
        "    num_applicant_selectors = [\n",
        "        \"span.applicant-rank-header-text\",\n",
        "        \"table.other-applicants-table.comparison-table tr td\",\n",
        "        \"p.number-of-applicants\"\n",
        "    ]\n",
        "    for selector in num_applicant_selectors:\n",
        "        try:\n",
        "            num_applicants = driver.find_element_by_css_selector(selector).text\n",
        "        except Exception as e:\n",
        "            pass\n",
        "        else:\n",
        "            return ''.join(list(filter(lambda c: c.isdigit(), num_applicants)))\n",
        "    return ''\n",
        "\n",
        "def applicants_education(driver):\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    return dictionary of applicant education levels\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    education_selector = \"table.applicants-education-table.comparison-table tbody tr\"\n",
        "    \n",
        "    try:\n",
        "        education = driver.find_elements_by_css_selector(education_selector)\n",
        "        if education:\n",
        "            # grab the degree type and proportion of applicants with that\n",
        "            # degree.\n",
        "            remove = [\"have\", \"a\", \"Degree\", \"degrees\", \"(Similar\", \"to\", \"you)\"]\n",
        "            edu_map = list(map(\n",
        "                    lambda edu: list(filter(\n",
        "                            lambda word: word not in remove, \n",
        "                            edu\n",
        "                        )), \n",
        "                    [item.text.split() for item in education]\n",
        "                ))\n",
        "            # store the education levels in a dictionary and prepare to \n",
        "            # write it to file\n",
        "            edu_dict = {\n",
        "                \"education\" + str(i + 1) : { \n",
        "                                    \"degree\" : ' '.join(edu_map[i][1:]), \n",
        "                                    \"proportion\": edu_map[i][0]\n",
        "                                } \n",
        "                for i in range(len(edu_map))\n",
        "            }\n",
        "            return edu_dict\n",
        "    except Exception as e:\n",
        "        print(\"error acquiring applicants education\")\n",
        "        print(e)\n",
        "    return {}\n",
        "\n",
        "def applicants_locations(driver):\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    scrapes the applicants-insights-hover-content div on a \n",
        "    given job page. Grabs the location and number of applicants \n",
        "    from each location.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    applicants_info = {}\n",
        "    try:\n",
        "        elem = driver.find_elements_by_css_selector(\"a.location-title\")\n",
        "        for i in range(len(elem)):\n",
        "            # city and applicants are separated by a new line\n",
        "            city, applicants = elem[i].text.split('\\n')\n",
        "            # get number of applicants by removing the word 'applicants'\n",
        "            applicants = applicants[:applicants.find(\" applicants\")]\n",
        "            # enter, typically, three applicant location data pairs\n",
        "            location_data  = {\n",
        "                \"city\"       : city, \n",
        "                \"applicants\" : applicants\n",
        "            }\n",
        "            applicants_info[\"location\" + str(i + 1)] = location_data\n",
        "    except Exception as e:\n",
        "        print(\"error acquiring applicants locations\")\n",
        "        print(e)\n",
        "    return applicants_info\n",
        "\n",
        "def applicants_skills(driver):\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    scrapes applicant skills by finding 'pill' tags in html\n",
        "    returns list of skills. If skills not present on page, then\n",
        "    returns empty list\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        raw_skills = driver.find_elements_by_css_selector(\"span.pill\")\n",
        "        skills     = [skill.text for skill in raw_skills] \n",
        "        return skills\n",
        "    except Exception as e:\n",
        "        print(\"error acquiring applicant skills\")\n",
        "        print(e)\n",
        "    return []\n",
        "\n",
        "def scrape_page(driver, **kwargs):\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    scrapes single job page after the driver loads a new job posting.\n",
        "    Returns data as a dictionary\n",
        "    \n",
        "    \"\"\"\n",
        "    # wait ~1 second for elements to be dynamically rendered\n",
        "    time.sleep(1.2)\n",
        "    start = time.time()\n",
        "    containers = [\n",
        "        \"section#top-card div.content\",            # job content\n",
        "        \"div.job-salary-container\",                # job salary\n",
        "        \"ul.company-growth-stats.stats-list\",      # company stats\n",
        "        \"div.insights-card.applicants-skills\",     # applicants skills\n",
        "        \"div.applicants-locations-list\"            # applicants locations\n",
        "    ]\n",
        "    for container in containers:\n",
        "        try:\n",
        "            WebDriverWait(driver, .25).until(\n",
        "                EC.presence_of_element_located(\n",
        "                    (By.CSS_SELECTOR, container)\n",
        "                    )\n",
        "                )\n",
        "        except Exception as e:\n",
        "            print(\"timeout error waiting for container to load or element\" \\\n",
        "                  \" not found: {}\".format(container))\n",
        "            print(e)\n",
        "    applicant_info = {\n",
        "        \"num_applicants\"    :  num_applicants(driver),\n",
        "        \"skills\"            :  applicants_skills(driver),\n",
        "        \"education\"         :  applicants_education(driver),\n",
        "        \"locations\"         :  applicants_locations(driver)\n",
        "    }\n",
        "    job_info = {\n",
        "        \"job_id\"            :  job_id(driver),\n",
        "        \"salary_estimates\"  :  salary_data(driver),\n",
        "        \"company_info\"      :  company_data(driver)\n",
        "    }\n",
        "    \n",
        "    job_info.update(job_data(driver))\n",
        "    \n",
        "    data = {\n",
        "        \"applicant_info\"    :  applicant_info,\n",
        "        \"job_info\"          :  job_info,\n",
        "        \"post_info\"         :  post_data(driver),\n",
        "        \"search_info\"       :  kwargs\n",
        "    }\n",
        "    print(\"scraped page in  {}  seconds\\n\".format(time.time()-start))\n",
        "    \n",
        "    # try:\n",
        "    #     print(\"data:\\n\\n{}\\n\".format(data))\n",
        "    # except Exception as e:\n",
        "    #     print(\"data could not be printed to console\\n\")\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UKOeZ9Nfarp-",
        "colab": {}
      },
      "source": [
        "def write_line_to_file(filename, data):\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    output the current job title, company, job id, then write\n",
        "    the scraped data to file\n",
        "    \n",
        "    \"\"\"\n",
        "    job_title = data[\"job_info\"][\"job_title\"]\n",
        "    company   = data[\"job_info\"][\"company\"]\n",
        "    job_id    = data[\"job_info\"][\"job_id\"]\n",
        "    message = u\"Writing data to file for job listing:\"\n",
        "    message += \"\\n  {}  {};   job id  {}\\n\"\n",
        "    try:\n",
        "        print(message.format(job_title, company, job_id))\n",
        "    except Exception as e:\n",
        "        print(\"Encountered a unicode encode error while attempting to print \" \\\n",
        "              \"the job post information;  job id {}\".format(job_id))\n",
        "    with open(filename, \"a\") as f:\n",
        "        f.write(json.dumps(data) + '\\n')\n",
        "\n",
        "def get_date_time():\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    get the full date along with the hour of the search, just for \n",
        "    completeness. Allows us to solve for of the original post \n",
        "    date.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    now   =  datetime.datetime.now()\n",
        "    month =  str(now.month) if now.month > 6 else '0' + str(now.month)\n",
        "    day   =  str(now.day) if now.day > 6 else '0' + str(now.day)\n",
        "    date  =  ''.join(str(t) for t in [now.year, month, day, now.time().hour])\n",
        "    return date\n",
        "\n",
        "def adjust_date_range(driver, date_range):\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    select a specific date range for job postings\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    if date_range == 'All':\n",
        "        return\n",
        "    index = ['', 'All', '1', '2-7', '8-14', '15-30'].index(date_range)\n",
        "    button_path = \"html/body/div[3]/div/div[2]/div[1]/div[4]/form/div/ul/li\" \\\n",
        "                  \"[3]/fieldset/button\"\n",
        "    date_path = \"html/body/div[3]/div/div[2]/div[1]/div[4]/form/div/ul/li\" \\\n",
        "                \"[3]/fieldset/div/ol/li[{}]/div/label\".format(index)\n",
        "    attempts = 1\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            elem = driver.find_element_by_xpath(button_path)\n",
        "            time.sleep(3)\n",
        "        except Exception as e:\n",
        "            attempts += 1\n",
        "            if attempts > 25:\n",
        "                break\n",
        "        else:\n",
        "            elem.click()\n",
        "            time.sleep(3)\n",
        "            driver.find_element_by_xpath(date_path).click()\n",
        "            time.sleep(3)\n",
        "            break \n",
        "\n",
        "def adjust_search_radius(driver, search_radius):\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    select the appropriate user-defined search radius from the \n",
        "    dropdown window\n",
        "    \n",
        "    \"\"\"\n",
        "    if search_radius == '50':\n",
        "        return\n",
        "    distance_selector = \"select#advs-distance > option[value='{}']\"\n",
        "    distance_selector = distance_selector.format(search_radius)\n",
        "    \n",
        "    try:\n",
        "        driver.find_element_by_css_selector(distance_selector).click()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        \n",
        "    else:\n",
        "        time.sleep(3)\n",
        "        \n",
        "        try:\n",
        "            driver.find_element_by_css_selector(\"input.submit-advs\").click()\n",
        "            time.sleep(3)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "def adjust_salary_range(driver, salary):\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    adjust the salary range, default is All salaries\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    if salary == 'All': \n",
        "        return\n",
        "    index = ['', 'All', '40+', '60+', '80+', '100+', \n",
        "                        '120+', '160+', '180+', '200+'].index(salary)\n",
        "    salary_button = \"html/body/div[3]/div/div[2]/div[1]/div[4]/form/div/ul/\" \\\n",
        "                                    \"li[4]/fieldset/button\"\n",
        "    salary_path = \"html/body/div[3]/div/div[2]/div[1]/div[4]/\" \\\n",
        "                  \"form/div/ul/li[4]/fieldset/div[1]/ol/li[{}\" \\\n",
        "                  \"]/div/label\".format(index)\n",
        "    attempts = 1\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            elem = driver.find_element_by_xpath(salary_button)\n",
        "            time.sleep(3)\n",
        "            \n",
        "        except Exception as e:\n",
        "            attempts += 1\n",
        "            if attempts > 25: \n",
        "                break\n",
        "        else:\n",
        "            elem.click()\n",
        "            time.sleep(3)\n",
        "            driver.find_element_by_xpath(salary_path).click()\n",
        "            break\n",
        "\n",
        "def sort_results_by(driver, sorting_criteria):\n",
        "    \n",
        "    \"\"\"\n",
        "    sort results by either relevance or date posted\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    if sorting_criteria.lower() == 'relevance':\n",
        "        \n",
        "        return\n",
        "    button = '//select[@id=\"jserp-sort-select\"]'\n",
        "    option_path = '//option[@value=\"DD\"]'\n",
        "    time.sleep(3)\n",
        "    \n",
        "    try:\n",
        "        driver.find_element_by_xpath(button).click()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(\" Could not sort results by '{}'\".format(sorting_criteria))\n",
        "        \n",
        "    else:\n",
        "        time.sleep(3)\n",
        "        \n",
        "        try:\n",
        "            driver.find_element_by_xpath(option_path).click()\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(\"  Could not select 'sort by' option\")\n",
        "            \n",
        "        else:\n",
        "            time.sleep(3)\n",
        "\n",
        "def robust_wait_for_clickable_element(driver, delay, selector):\n",
        "    \n",
        "    \"\"\"\n",
        "    wait for css selector to load \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    clickable = False\n",
        "    attempts = 1\n",
        "    try:\n",
        "        driver.find_element_by_xpath(selector)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(\"Selector not found: {}\".format(selector))\n",
        "        \n",
        "    else:\n",
        "        \n",
        "        while not clickable:\n",
        "            \n",
        "            try:\n",
        "                # wait for job post link to load\n",
        "                wait_for_clickable_element(driver, delay, selector)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(\"  {}\".format(e))\n",
        "                attempts += 1\n",
        "                if attempts % 100 == 0:\n",
        "                    driver.refresh()\n",
        "                if attempts > 10**3: \n",
        "                    print(\"  \\nrobust_wait_for_clickable_element failed \" \\\n",
        "                                    \"after too many attempts\\n\")\n",
        "                    break\n",
        "                pass\n",
        "            \n",
        "            else:\n",
        "                clickable = True\n",
        "\n",
        "def robust_click(driver, delay, selector):\n",
        "    \"\"\"\n",
        "    \n",
        "    use a while-looop to click an element. For stubborn links\n",
        "    and general unexpected browser errors.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        driver.find_element_by_xpath(selector).click()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(\"  The job post link was likely hidden,\\n    An \" \\\n",
        "                \"error was encountered while attempting to click link\" \\\n",
        "                \"\\n    {}\".format(e))\n",
        "        attempts = 1\n",
        "        clicked = False\n",
        "        \n",
        "        while not clicked:\n",
        "            try:\n",
        "                driver.find_element_by_xpath(selector).click()\n",
        "                \n",
        "            except Exception as e:\n",
        "                pass\n",
        "            \n",
        "            else:\n",
        "                clicked = True\n",
        "                print(\"  Successfully navigated to job post page \"\\\n",
        "                            \"after {} attempts\".format(attempts))\n",
        "            finally:\n",
        "                attempts += 1\n",
        "                \n",
        "                if attempts % 100 == 0:\n",
        "                    print(\"--------------  refreshing page\")\n",
        "                    driver.refresh()\n",
        "                    time.sleep(5)\n",
        "                if attempts > 10**3:\n",
        "                    print(selector)\n",
        "                    print(\"  robust_click method failed after too many attempts\")\n",
        "                    break \n",
        "\n",
        "\n",
        "def wait_for_clickable_element(driver, delay, selector):\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    use WebDriverWait to wait for an element to become clickable\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    obj = WebDriverWait(driver, delay).until(\n",
        "            EC.element_to_be_clickable(\n",
        "                (By.XPATH, selector)\n",
        "            )\n",
        "        )\n",
        "    return obj  \n",
        "\n",
        "def wait_for_clickable_element_css(driver, delay, selector):\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    use WebDriverWait to wait for an element to become clickable\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    obj = WebDriverWait(driver, delay).until(\n",
        "            EC.element_to_be_clickable(\n",
        "                (By.CSS_SELECTOR, selector)\n",
        "            )\n",
        "        )\n",
        "    return obj  \n",
        "\n",
        "\n",
        "def link_is_present(driver, delay, selector, index, results_page):\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    verify that the link selector is present and print the search \n",
        "    details to console. This method is particularly useful for catching\n",
        "    the last link on the last page of search results\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        WebDriverWait(driver, delay).until(\n",
        "            EC.presence_of_element_located(\n",
        "                (By.XPATH, selector)\n",
        "            )\n",
        "        )\n",
        "        print(\"**************************************************\")\n",
        "        print(\"\\nScraping data for result  {}\" \\\n",
        "                \"  on results page  {} \\n\".format(index, results_page))\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        if index < 25:\n",
        "            print(\"\\nWas not able to wait for job_selector to load. Search \" \\\n",
        "                    \"results may have been exhausted.\")\n",
        "            return True\n",
        "        \n",
        "        else:\n",
        "            return False\n",
        "        \n",
        "    else:\n",
        "        return True \n",
        "\n",
        "\n",
        "def search_suggestion_box_is_present(driver, selector, index, results_page):\n",
        "    \"\"\"\n",
        "    \n",
        "    check results page for the search suggestion box,\n",
        "    as this causes some errors in navigate search results.\n",
        "    \n",
        "    \"\"\"\n",
        "    if (index == 1) and (results_page == 1):\n",
        "        try:\n",
        "            # This try-except statement allows us to avoid the \n",
        "            # problems cause by the LinkedIn search suggestion box\n",
        "            driver.find_element_by_css_selector(\"div.suggested-search.bd\")\n",
        "        except Exception as e:\n",
        "            pass\n",
        "        else:\n",
        "            return True\n",
        "    else:\n",
        "        return False\n",
        "    \n",
        "\n",
        "def next_results_page(driver, delay):\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    navigate to the next page of search results. If an error is encountered\n",
        "    then the process ends or new search criteria are entered as the current \n",
        "    search results may have been exhausted.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        # wait for the next page button to load\n",
        "        print(\"  Moving to the next page of search results... \\n\" \\\n",
        "                \"  If search results are exhausted, will wait {} seconds \" \\\n",
        "                \"then either execute new search or quit\".format(delay))\n",
        "        wait_for_clickable_element_css(driver, delay, \"a.next-btn\")\n",
        "        # navigate to next page\n",
        "        driver.find_element_by_css_selector(\"a.next-btn\").click()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print (\"\\nFailed to click next page link; Search results \" \\\n",
        "                                \"may have been exhausted\\n{}\".format(e))\n",
        "        raise ValueError(\"Next page link not detected; search results exhausted\")\n",
        "        \n",
        "    else:\n",
        "        # wait until the first job post button has loaded\n",
        "        first_job_button = \"a.job-title-link\"\n",
        "        # wait for the first job post button to load\n",
        "        wait_for_clickable_element_css(driver, delay, first_job_button)\n",
        "\n",
        "def go_to_specific_results_page(driver, delay, results_page):\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    go to a specific results page in case of an error, can restart \n",
        "    the webdriver where the error occurred.\n",
        "    \n",
        "    \"\"\"\n",
        "    if results_page < 2:\n",
        "        return\n",
        "    current_page = 1\n",
        "    for i in range(results_page):\n",
        "        current_page += 1\n",
        "        time.sleep(5)\n",
        "        try:\n",
        "            next_results_page(driver, delay)\n",
        "            print(\"\\n**************************************************\")\n",
        "            print(\"\\n\\n\\nNavigating to results page {}\" \\\n",
        "                  \"\\n\\n\\n\".format(current_page))\n",
        "        except ValueError:\n",
        "            print(\"**************************************************\")\n",
        "            print(\"\\n\\n\\n\\n\\nSearch results exhausted\\n\\n\\n\\n\\n\")\n",
        "\n",
        "def print_num_search_results(driver, keyword, location):\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    print the number of search results to console\n",
        "    \n",
        "    \"\"\"\n",
        "    # scroll to top of page so first result is in view\n",
        "    driver.execute_script(\"window.scrollTo(0, 0);\")\n",
        "    selector = \"div.results-context div strong\"\n",
        "    try:\n",
        "        num_results = driver.find_element_by_css_selector(selector).text\n",
        "    except Exception as e:\n",
        "        num_results = ''\n",
        "    print(\"**************************************************\")\n",
        "    print(\"\\n\\n\\n\\n\\nSearching  {}  results for  '{}'  jobs in  '{}' \" \\\n",
        "            \"\\n\\n\\n\\n\\n\".format(num_results, keyword, location))\n",
        "\n",
        "def extract_transform_load(driver, delay, selector, date, \n",
        "                           keyword, location, filename):\n",
        "    \"\"\"\n",
        "    \n",
        "    using the next job posting selector on a given results page, wait for\n",
        "    the link to become clickable, then navigate to it. Wait for the job \n",
        "    posting page to load, then scrape the page and write the data to file.\n",
        "    Finally, go back to the search results page\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    # wait for the job post to load then navigate to it\n",
        "    try:\n",
        "        wait_for_clickable_element(driver, delay, selector)\n",
        "        robust_click(driver, delay, selector)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(\"error navigating to job post page\")\n",
        "        print(e)\n",
        "        \n",
        "    try:\n",
        "        # wait for the premium applicant insights to load\n",
        "        # wait_for_clickable_element(driver, delay, \"div.premium-insights\")\n",
        "        WebDriverWait(driver, delay).until(\n",
        "            EC.presence_of_element_located(\n",
        "                (By.CSS_SELECTOR, \"div.premium-insights\")\n",
        "                )\n",
        "            )\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    try:\n",
        "        # scrape page and prepare to write the data to file\n",
        "        data = scrape_page(driver, keyword=keyword, location=location, dt=date)\n",
        "    except Exception as e:\n",
        "        print(\"\\nSearch results may have been exhausted. An error was \" \\\n",
        "                \"encountered while attempting to scrape page data\")\n",
        "        print(e)\n",
        "    else:\n",
        "        # write data to file\n",
        "        write_line_to_file(filename, data)\n",
        "    finally:\n",
        "        if not (\"Search | LinkedIn\" in driver.title):\n",
        "            driver.execute_script(\"window.history.go(-1)\")\n",
        "\n",
        "\n",
        "class LIClient(object):\n",
        "    \n",
        "    def __init__(self, driver, **kwargs):\n",
        "        self.driver         =  driver\n",
        "        self.username       =  kwargs[\"username\"]\n",
        "        self.password       =  kwargs[\"password\"]\n",
        "        self.filename       =  kwargs[\"filename\"]\n",
        "        self.date_range     =  kwargs[\"date_range\"]\n",
        "        self.search_radius  =  kwargs[\"search_radius\"]\n",
        "        self.sort_by        =  kwargs[\"sort_by\"]\n",
        "        self.salary_range   =  kwargs[\"salary_range\"]\n",
        "        self.results_page   =  kwargs[\"results_page\"]\n",
        "\n",
        "    def driver_quit(self):\n",
        "        self.driver.quit()\n",
        "\n",
        "    def login(self):\n",
        "        \n",
        "        \"\"\"\n",
        "        login to linkedin then wait 3 seconds for page to load\n",
        "        \"\"\"\n",
        "        # Enter login credentials\n",
        "        WebDriverWait(self.driver, 120).until(\n",
        "            EC.element_to_be_clickable(\n",
        "                (By.ID, \"session_username-login\")\n",
        "            )\n",
        "        )\n",
        "        elem = self.driver.find_element_by_id(\"session_username-login\")\n",
        "        elem.send_keys(self.username)\n",
        "        elem = self.driver.find_element_by_id(\"session_password-login\")\n",
        "        elem.send_keys(self.password)\n",
        "        # Enter credentials with Keys.RETURN\n",
        "        elem.send_keys(Keys.RETURN)\n",
        "        # Wait a few seconds for the page to load\n",
        "        time.sleep(3)\n",
        "\n",
        "    def navigate_to_jobs_page(self):\n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        navigate to the 'Jobs' page since it is a convenient page to \n",
        "        enter a custom job search.\n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        # Click the Jobs search page\n",
        "        jobs_link_clickable = False\n",
        "        attempts = 1\n",
        "        url = \"https://www.linkedin.com/jobs/?trk=nav_responsive_sub_nav_jobs\"\n",
        "        while not jobs_link_clickable:\n",
        "            try:\n",
        "                self.driver.get(url)\n",
        "            except Exception as e:\n",
        "                attempts += 1\n",
        "                if attempts > 10**3: \n",
        "                    print(\"  jobs page not detected\")\n",
        "                    break\n",
        "                pass\n",
        "            else:\n",
        "                print(\"**************************************************\")\n",
        "                print (\"\\n\\n\\nSuccessfully navigated to jobs search page\\n\\n\\n\")\n",
        "                jobs_link_clickable = True\n",
        "\n",
        "    def enter_search_keys(self):\n",
        "        \n",
        "        \"\"\"\n",
        "        execute the job search by entering job and location information.\n",
        "        The location is pre-filled with text, so we must clear it before\n",
        "        entering our search.\n",
        "        \n",
        "        \"\"\"\n",
        "        driver = self.driver\n",
        "        WebDriverWait(driver, 120).until(\n",
        "            EC.presence_of_element_located(\n",
        "                (By.ID, \"keyword-search-box\")\n",
        "            )\n",
        "        )\n",
        "        # Enter search criteria\n",
        "        elem = driver.find_element_by_id(\"keyword-search-box\")\n",
        "        elem.send_keys(self.keyword)\n",
        "        # clear the text in the location box then enter location\n",
        "        elem = driver.find_element_by_id(\"location-search-box\").clear()\n",
        "        elem = driver.find_element_by_id(\"location-search-box\")\n",
        "        elem.send_keys(self.location)\n",
        "        elem.send_keys(Keys.RETURN)\n",
        "        time.sleep(3)\n",
        "\n",
        "    def customize_search_results(self):\n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        sort results by either relevance or date posted\n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        adjust_date_range(self.driver, self.date_range)\n",
        "        adjust_salary_range(self.driver, self.salary_range)\n",
        "        # adjust_search_radius(self.driver, self.search_radius) # deprecated\n",
        "        # scroll to top of page so the sorting menu is in view\n",
        "        self.driver.execute_script(\"window.scrollTo(0, 0);\")\n",
        "        sort_results_by(self.driver, self.sort_by)\n",
        "\n",
        "    def navigate_search_results(self):\n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        scrape postings for all pages in search results\n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        driver = self.driver\n",
        "        search_results_exhausted = False\n",
        "        results_page = self.results_page\n",
        "        delay = 60\n",
        "        date = get_date_time()\n",
        "        # css elements to view job pages\n",
        "        list_element_tag = '/descendant::a[@class=\"job-title-link\"]['\n",
        "        print_num_search_results(driver, self.keyword, self.location)\n",
        "        # go to a specific results page number if one is specified\n",
        "        go_to_specific_results_page(driver, delay, results_page)\n",
        "        results_page = results_page if results_page > 1 else 1\n",
        "\n",
        "        while not search_results_exhausted:\n",
        "            for i in range(1,26):  # 25 results per page\n",
        "                # define the css selector for the blue 'View' button for job i\n",
        "                job_selector = list_element_tag + str(i) + ']'\n",
        "                if search_suggestion_box_is_present(driver, \n",
        "                                            job_selector, i, results_page):\n",
        "                    continue\n",
        "                # wait for the selector for the next job posting to load.\n",
        "                # if on last results page, then throw exception as job_selector \n",
        "                # will not be detected on the page\n",
        "                if not link_is_present(driver, delay, \n",
        "                                         job_selector, i, results_page):\n",
        "                    continue\n",
        "                robust_wait_for_clickable_element(driver, delay, job_selector)\n",
        "                extract_transform_load(driver,\n",
        "                                       delay,\n",
        "                                       job_selector,\n",
        "                                       date,\n",
        "                                       self.keyword,\n",
        "                                       self.location,\n",
        "                                       self.filename)\n",
        "            # attempt to navigate to the next page of search results\n",
        "            # if the link is not present, then the search results have been \n",
        "            # exhausted\n",
        "            \n",
        "            try:\n",
        "                next_results_page(driver, delay)\n",
        "                print(\"\\n**************************************************\")\n",
        "                print(\"\\n\\n\\nNavigating to results page  {}\" \\\n",
        "                      \"\\n\\n\\n\".format(results_page + 1))\n",
        "            except ValueError:\n",
        "                search_results_exhausted = True\n",
        "                print(\"**************************************************\")\n",
        "                print(\"\\n\\n\\n\\n\\nSearch results exhausted\\n\\n\\n\\n\\n\")\n",
        "            else:\n",
        "                results_page += 1\n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eUkwccoYarp0",
        "colab": {}
      },
      "source": [
        "def parse_command_line_args():\n",
        "    \"\"\"\n",
        "                                     \n",
        "    parse LinkedIn search parameters\n",
        "        \n",
        "    \"\"\"\n",
        "    \n",
        "    parser = argparse.ArgumentParser()\n",
        "    \n",
        "    parser.add_argument('--username', default=search_keys['username'], nargs='*',\n",
        "        help=\"\"\"\n",
        "        \n",
        "        enter LI username\n",
        "        \n",
        "        \"\"\")\n",
        "        \n",
        "    parser.add_argument('--password', default=search_keys['password'], nargs='*',\n",
        "        help=\"\"\"\n",
        "        \n",
        "        enter LI password\"\"\")\n",
        "        \n",
        "    parser.add_argument('--keywords', default=search_keys['keywords'], nargs='*', \n",
        "        help=\"\"\"\n",
        "        \n",
        "        enter search keys separated by a single space. If the keyword is more\n",
        "        than one word, wrap the keyword in double quotes.\n",
        "        \n",
        "        \"\"\")\n",
        "        \n",
        "    parser.add_argument('--locations', default=search_keys['locations'], nargs='*',\n",
        "        help=\"\"\"\n",
        "        \n",
        "        enter search locations separated by a single space. If the location \n",
        "        search is more than one word, wrap the location in double quotes.\n",
        "        \n",
        "        \"\"\")\n",
        "        \n",
        "    parser.add_argument('--search_radius', type=int, default=search_keys['search_radius'], nargs='?',\n",
        "        help=\"\"\"\n",
        "        \n",
        "        enter a search radius (in miles). Possible values are: 10, 25, 35, \n",
        "        50, 75, 100. Defaults to 50.\n",
        "        \n",
        "        \"\"\")\n",
        "        \n",
        "    parser.add_argument('--results_page', type=int, default=search_keys['page_number'], nargs='?', \n",
        "        help=\"\"\"\n",
        "        \n",
        "        enter a specific results page. If an unexpected error occurs, one can\n",
        "        resume the previous search by entering the results page where they \n",
        "        left off. Defaults to first results page.\n",
        "        \n",
        "        \"\"\")\n",
        "    \n",
        "    parser.add_argument('--date_range', type=str, default=search_keys['date_range'], nargs='?', \n",
        "        help=\n",
        "       \"\"\"\n",
        "        \n",
        "        specify a specific date range. Possible values are: All, 1, 2-7, 8-14,\n",
        "        15-30. Defaults to 'All'.\n",
        "        \n",
        "        \"\"\")\n",
        "    \n",
        "    parser.add_argument('--sort_by', type=str, default=search_keys['sort_by'], nargs='?', \n",
        "        help=\n",
        "        \"\"\"\n",
        "        \n",
        "        sort results by relevance or date posted. If the input string is not \n",
        "        equal to 'Relevance' (case insensitive), then results will be sorted \n",
        "        by date posted. Defaults to sorting by relevance.\n",
        "        \n",
        "        \"\"\")\n",
        "    \n",
        "    parser.add_argument('--salary_range', type=str, default=search_keys['salary_range'], nargs='?', \n",
        "        help=\"\"\"\n",
        "        \n",
        "        set a minimum salary requirement. Possible input values are:\n",
        "        All, 40+, 60+, 80+, 100+, 120+, 140+, 160+, 180+, 200+. Defaults\n",
        "        to All.\n",
        "        \n",
        "        \"\"\")\n",
        "    \n",
        "    parser.add_argument('--filename', type=str, default=search_keys['filename'], nargs='?', \n",
        "        help=\"\"\"\n",
        "        \n",
        "        specify a filename to which data will be written.\n",
        "        \n",
        "        \"\"\")\n",
        "    \n",
        "    return vars(parser.parse_args())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    search_keys = parse_command_line_args()\n",
        "\n",
        "    # initialize selenium webdriver - pass latest chromedriver path to webdriver.Chrome()\n",
        "    driver = webdriver.Chrome('/usr/bin/chromedriver')\n",
        "    driver.get(\"https://www.linkedin.com/uas/login\")\n",
        "\n",
        "    # initialize LinkedIn web client\n",
        "    liclient = LIClient(driver, **search_keys)\n",
        "\n",
        "    liclient.login()\n",
        "\n",
        "    # wait for page load\n",
        "    time.sleep(3)\n",
        "\n",
        "    assert isinstance(search_keys[\"keywords\"], list)\n",
        "    assert isinstance(search_keys[\"locations\"], list)\n",
        "\n",
        "    for keyword in search_keys[\"keywords\"]:\n",
        "        for location in search_keys[\"locations\"]:\n",
        "            liclient.keywords = keywords\n",
        "            liclient.locations = locations\n",
        "            liclient.navigate_to_jobs_page()\n",
        "            liclient.enter_search_keys()\n",
        "            liclient.customize_search_results()\n",
        "            liclient.navigate_search_results()\n",
        "\n",
        "liclient.driver_quit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IB7MoIfZarqO",
        "colab": {}
      },
      "source": [
        "# initialize Selenium Web Driver\n",
        "driver = webdriver.Chrome()\n",
        "driver.get(\"https://www.linkedin.com/uas/login\")\n",
        "\n",
        "# initialize LinkedIn web client\n",
        "liclient = LIClient(driver, **search_keys)\n",
        "liclient.login()\n",
        "\n",
        "# wait for page load\n",
        "time.sleep(3)\n",
        "liclient.keyword  = \"software\"\n",
        "liclient.location = \"'rio de janeiro', 'são paulo'\"\n",
        "liclient.navigate_to_jobs_page()\n",
        "liclient.enter_search_keys()\n",
        "liclient.customize_search_results()\n",
        "liclient.navigate_search_results()\n",
        "liclient.driver_quit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvcgBLElmsJn",
        "colab_type": "text"
      },
      "source": [
        "# **IBGE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE9vCeSumsJt",
        "colab_type": "text"
      },
      "source": [
        "The Brazilian Institute of Geography and Statistics or IBGE is the agency responsible for official collection of statistical, geographic, cartographic, geodetic and environmental information in Brazil. \n",
        "\n",
        "(from Wikipedia)\n",
        "\n",
        "![imagem.png](attachment:imagem.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRyHlvV2msJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "desemprego19 = pd.read_csv('desemprego2019.csv', encoding = 'latin-1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhVZ74A9msKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "desemprego20 = pd.read_csv('desemprego2020.csv', encoding = 'latin-1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ_Fg4qjmsKR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Join two Data Frames\n",
        "\n",
        "porcentagem = pd.DataFrame({'Ano': ['2019', '2020'],\n",
        "                           'Taxa de Ocupação': ['87.28%', '87.77%'],\n",
        "                           'Taxa de Desocupação': ['12.72%', '12.72%']\n",
        "                           })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLW6wIAzmsKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "porcentagem.to_csv('porcentagem.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYsVheBWmsK3",
        "colab_type": "code",
        "colab": {},
        "outputId": "12e4eb36-b284-4c54-bb52-a079d92e2013"
      },
      "source": [
        "pd.read_csv('porcentagem.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ano</th>\n",
              "      <th>Taxa de Ocupação</th>\n",
              "      <th>Taxa de Desocupação</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2019</td>\n",
              "      <td>87.28%</td>\n",
              "      <td>12.72%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020</td>\n",
              "      <td>87.77%</td>\n",
              "      <td>12.72%</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Ano Taxa de Ocupação Taxa de Desocupação\n",
              "0  2019           87.28%              12.72%\n",
              "1  2020           87.77%              12.72%"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzpOhvHAmsLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "empregados = pd.read_excel('Tabela 6466.xlsx', encoding = 'latin-1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqBkocshmsLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Cleaning empregados df\n",
        "\n",
        "# Tabela mostra o nível da ocupação, na semana de referência, das pessoas de 14 anos ou mais de idade \n",
        "# Total, coeficiente de variação, variações em relação ao trimestre anterior e ao mesmo trimestre \n",
        "# do ano anterior, e média anual\n",
        "\n",
        "# Drop rows that all values are missing\n",
        "empregados.dropna(how='any', inplace=True) \n",
        "\n",
        "# Rename columns names\n",
        "empregados.rename({\n",
        "        'Tabela 6466 - Nível da ocupação, na semana de referência, das pessoas de 14 anos ou mais de idade - Total, coeficiente de variação, variações em relação ao trimestre anterior e ao mesmo trimestre do ano anterior, e média anual': 'Region', \n",
        "        'Unnamed: 1': '1º trimestre 2018', 'Unnamed: 2': '4º trimestre 2019 ', \n",
        "        'Unnamed: 3': '1º trimestre 2020' \n",
        "        }, axis='columns', inplace=True)\n",
        "\n",
        "empregados.to_csv('empregados.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEisOCJlmsLq",
        "colab_type": "code",
        "colab": {},
        "outputId": "a74cf2a2-3e29-48ea-bbfc-9800ff15265b"
      },
      "source": [
        "pd.read_csv('empregados.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Region</th>\n",
              "      <th>1º trimestre 2018</th>\n",
              "      <th>4º trimestre 2019</th>\n",
              "      <th>1º trimestre 2020</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pará</td>\n",
              "      <td>51.8</td>\n",
              "      <td>53.0</td>\n",
              "      <td>52.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Rio de Janeiro</td>\n",
              "      <td>51.6</td>\n",
              "      <td>51.9</td>\n",
              "      <td>50.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>São Paulo</td>\n",
              "      <td>58.1</td>\n",
              "      <td>59.7</td>\n",
              "      <td>58.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Region  1º trimestre 2018  4º trimestre 2019   1º trimestre 2020\n",
              "0            Pará               51.8                53.0               52.2\n",
              "1  Rio de Janeiro               51.6                51.9               50.8\n",
              "2       São Paulo               58.1                59.7               58.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUu_6yOamsL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "desempregados = pd.read_excel('Tabela 6467.xlsx', encoding = 'latin-1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxoNFvB2msL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Cleaning desempregados df\n",
        "\n",
        "# Tabela que mostra o nível da desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n",
        "# Total, coeficiente de variação, variações em relação ao trimestre anterior e ao mesmo trimestre do ano \n",
        "# anterior, e média anual\n",
        "\n",
        "# Drop rows that all values are missing\n",
        "desempregados.dropna(how='any', inplace=True) \n",
        "\n",
        "# Rename columns names\n",
        "desempregados.rename({\n",
        "        'Tabela 6467 - Nível da desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade - Total, coeficiente de variação, variações em relação ao trimestre anterior e ao mesmo trimestre do ano anterior, e média anual': 'Region', \n",
        "        'Unnamed: 1': '1º trimestre 2018', 'Unnamed: 2': '4º trimestre 2019 ', \n",
        "        'Unnamed: 3': '1º trimestre 2020' \n",
        "        }, axis='columns', inplace=True)\n",
        "\n",
        "desempregados.to_csv('desempregados.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-ipAVEsmsMM",
        "colab_type": "code",
        "colab": {},
        "outputId": "bca19ada-5108-4f29-8420-d0911837f2a9"
      },
      "source": [
        "pd.read_csv('desempregados.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Region</th>\n",
              "      <th>1º trimestre 2018</th>\n",
              "      <th>4º trimestre 2019</th>\n",
              "      <th>1º trimestre 2020</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pará</td>\n",
              "      <td>6.7</td>\n",
              "      <td>5.4</td>\n",
              "      <td>6.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Rio de Janeiro</td>\n",
              "      <td>9.3</td>\n",
              "      <td>8.3</td>\n",
              "      <td>8.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>São Paulo</td>\n",
              "      <td>9.1</td>\n",
              "      <td>7.7</td>\n",
              "      <td>8.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Region  1º trimestre 2018  4º trimestre 2019   1º trimestre 2020\n",
              "0            Pará                6.7                 5.4                6.2\n",
              "1  Rio de Janeiro                9.3                 8.3                8.6\n",
              "2       São Paulo                9.1                 7.7                8.1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43zlzXIMmsMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}